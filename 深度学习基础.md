# 深度学习基础
## CNN基础
* 1.什么是卷积?

  对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个 权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的卷积操作.
>>>>>![卷积](https://wx4.sinaimg.cn/mw690/67111f6aly1fhykes5sffg207g05gdgf.gif)
* 2.卷积的作用？
  * 局部感知

    卷积核的大小一般小于输入图像的大小（如果等于则是全连接），因此卷积提取出的特征会更多地关注局部，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。
  * 参数共享

    最大作用是减少运算量。
  * 多核

    一个核选取的特征是固定的，多核选取不同的特征。
* 3.卷积的padding和stride

  * padding

    为了改变经过卷积后的feature map的大小，一般在输入feature map的四周填充0。
  * stride

    滑动步长，滑动窗口在输入feature map上每次移动的距离。
* 4.卷积的参数计算

  * 输出feature map大小计算

    假设输入feature map为h×w,卷积核大小为k×k,步长stride为s,padding为p。则输出feature map为(h-k+2p)/s+1, (w-k+2p)/s+1

  * 卷积核参数计算

    假设某卷积层卷积核为k×k×c×d(c为输入通道数,d为输出通道数),则该卷积层的参数量为(k×k×c+1)×d,+1是每个卷积核的bias(如果有的话)。
* 5.什么是CNN的感受野？如何计算感受野？

  * 感受野

    在卷积神经网络CNN中,决定某一层输出结果中一个元素所对应的输入层的区域大小,被称作感受野。

  * 如何计算

    (out-1)×s+k  out是output feature map大小,该方法与计算输出feature map相对应。
* 6.关于池化(pooling)

![pooling](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1535480114802&di=bbe4289085351a6319787d45748f9b83&imgtype=0&src=http%3A%2F%2Fxilinx.eetrend.com%2Ffiles-eetrend-xilinx%2Farticle%2F201610%2F10546-26351-juanjishenjingwangluozhongtuxiangchihuacaozuoquanjiexi.jpg)
  * 作用

    pooling一般在卷积层（conv）之后使用，使用pooling技术将卷积层后得到的小邻域内的特征点整合得到新的特征。一方面防止无用参数增加时间复杂度，一方面增加了特征的整合度。

    窗口滑动卷积的时候，卷积值就代表了整个窗口的特征。因为滑动的窗口间有大量重叠区域，出来的卷积值有冗余，进行最大pooling或者平均pooling就是减少冗余。

    具有一定程度上的平移和失真不变性。

    减小尺寸,提高运算速度的,同时也可以减少噪声。
