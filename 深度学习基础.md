# 深度学习基础
## CNN基础
* 1.什么是卷积?

  对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个 权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的卷积操作.
 >>>>>![卷积](https://wx4.sinaimg.cn/mw690/67111f6aly1fhykes5sffg207g05gdgf.gif)

* 2.卷积的作用？
  * 局部感知

   卷积核的大小一般小于输入图像的大小（如果等于则是全连接），因此卷积提取出的特征会更多地关注局部，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。
  * 参数共享

   最大作用是减少运算量。
  * 多核

    一个核选取的特征是固定的，多核选取不同的特征。
* 3.卷积的padding和stride

 * padding

   为了改变经过卷积后的feature map的大小，一般在输入feature map的四周填充0。
 * stride

   滑动步长，滑动窗口在输入feature map上每次移动的距离。
* 4.卷积的参数计算

  * 输出feature map大小计算

    假设输入feature map为h×w,卷积核大小为k×k,步长stride为s,padding为p。则输出feature map为(h-k+2p)/s+1, (w-k+2p)/s+1

  * 卷积核参数计算

    假设某卷积层卷积核为k×k×c×d(c为输入通道数,d为输出通道数),则该卷积层的参数量为(k×k×c+1)×d,+1是每个卷积核的bias(如果有的话)。
* 5.什么是CNN的感受野？如何计算感受野？

  * 感受野

    在卷积神经网络CNN中,决定某一层输出结果中一个元素所对应的输入层的区域大小,被称作感受野。

  * 如何计算

    (out-1)×s+k  out是output feature map大小,该方法与计算输出feature map相对应。
* 6.关于池化(pooling)

  ![pooling](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1535480114802&di=bbe4289085351a6319787d45748f9b83&imgtype=0&src=http%3A%2F%2Fxilinx.eetrend.com%2Ffiles-eetrend-xilinx%2Farticle%2F201610%2F10546-26351-juanjishenjingwangluozhongtuxiangchihuacaozuoquanjiexi.jpg)
  * 作用

    保留显著特征、降低特征维度，增大kernel的感受野。

    pooling一般在卷积层（conv）之后使用，使用pooling技术将卷积层后得到的小邻域内的特征点整合得到新的特征。一方面防止无用参数增加时间复杂度，一方面增加了特征的整合度。

    窗口滑动卷积的时候，卷积值就代表了整个窗口的特征。因为滑动的窗口间有大量重叠区域，出来的卷积值有冗余，进行最大pooling或者平均pooling就是减少冗余。

    具有一定程度上的平移和失真不变性。

    减小尺寸,提高运算速度的,同时也可以减少噪声。
* 7.全连接层的作用
  * 原理

   假设最后一层输出feature map大小为7×7×512，后一层为含有4096个神经元的全连接，则该层的参数为7×7×512×4096，相当于4096个大小为7×7×512的卷积核。

  * 作用

    信息整合，在网络中起到分类器的作用，两层及以上全连接可以很好地解决非线性的问题。

  * 缺点

    参数量极大(可占整个网络参数80%左右)，存在很大的冗余，训练起来速度慢，容易过拟合，全连接层限制了输入的大小必须保持一致。
* 8.global average pooling(GAP)

  ![GAP](https://leanote.com/api/file/getImage?fileId=596cddb4ab644114ba001f4e)https://leanote.com/api/file/getImage?fileId=596cddb4ab644114ba001f4e

  每个讲到全局池化的都会说GAP就是把avg pooling的窗口大小设置成feature map的大小，这虽然是正确的，但这并不是GAP内涵的全部。GAP的意义是对整个网络从结构上做正则化防止过拟合。既要参数少避免全连接带来的过拟合风险，又要能达到全连接一样的转换功能，怎么做呢？直接从feature map的通道上下手，如果我们最终有1000类，那么最后一层卷积输出的feature map就只有1000个channel，然后对这个feature map应用全局池化，输出长度为1000的向量，这就相当于剔除了全连接层黑箱子操作的特征，直接赋予了每个channel实际的类别意义。
* 9.梯度下降
  * 梯度

    二元函数Z=f(x,y) ,x方向上的偏导f'x,y方向上的偏导f'y,任意方向上的偏导为f'u = f'x cos(theta)+f'y sin(theta),theta是与x轴正方向的夹角。
    设两个向量F=(f'x,f'y) , I=(cos(theta),sin(theta)),则f'u = |F| |I| cosα，α是F和I的夹角，当同向时最大，某点的梯度为 f'x i + f'y j。沿着梯度方向，函数上升最快。沿梯度的反方向下降最快。

  * [各种梯度下降的方法(SGD momentum nestrov adagard rmsporp adadelta adam)](https://blog.csdn.net/bupt_wx/article/details/52761751)
* 10.反向传播算法

   简单来说，使用链式法则，求出损失函数对权重的梯度(损失函数对输出的梯度，输出对上一层输出的梯度，依次反向传播。。)，最终通过梯度下降法修改权重。

   [反向传播算法的例子](https://www.cnblogs.com/charlotte77/p/5629865.html)
* 11.损失函数
